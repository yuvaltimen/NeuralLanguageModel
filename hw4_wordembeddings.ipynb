{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Word Embeddings and Neural Language Models\n",
    "\n",
    "\n",
    "Names: __Suzanne Becker, Yuval Timen__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Word2Vec paper questions\n",
    "---------------------------\n",
    "\n",
    "### 1. Describe how a CBOW word embedding is generated.\n",
    "\n",
    "A CBOW word embedding can be created by using the CBOW algorithm. This would consist of training a classifier to predict a target word given some context words. Specifically, given the sequence of length M $w_{0} ... w_{M}$ and a window size N, we would first create one-hot vectors for each word in the sequence. These will be used as inputs to the classifier. Then for each time step $t$, we will be trying to predict the word $w_{t}$ given the context words within the N-sized window. We take the N words from either side of $w_{t}$, namely $w_{t-N}...w_{t+N}$ excluding $w_{t}$. These will each be fed through the forward pass of the classifier, which will give an output vector of size 1xV, representing the probability distribution of predicting the next word over our entire vocabulary. The error for each iteration in training will be the sum of the errors for all context words. We do the backward propagation, and once the weights have been updated, we move to the next timestep and shift our target word to be the next word in the sequence ($w_{t+1}$), and repeat the training steps; for each timestep, we run the forward pass, find the error, and do backpropagation.\n",
    "\n",
    "Once we complete training, our CBOW embeddings will be the weights we have learned. The weight matrix is of size VxE, where E is the size of the embeddings we want to learn. Thus we have V vectors each of length E, meaning that for each word in our vocabulary, we've created an embedding of length E.\n",
    "\n",
    "### 2. What is a CBOW word embedding and how is it different from a skip-gram word embedding?\n",
    "\n",
    "A CBOW word embedding is an embedding created using the CBOW algorithm - namely, embeddings learned by training a Neural Network to predict some target given some number of neighboring context words. This is very similar to the skip-gram method for generating word embeddings. The main difference is that, during the prediction task, CBOW tries to predict the target word given some neighboring context words, while skip-gram tries to predict the context words given a target word.\n",
    "\n",
    "### 3. What is the task that the authors use to evaluate the generated word embeddings?\n",
    "\n",
    "They tested that the word embeddings preserved the linear relationships between words; ideally, they wanted to have __vector(\"King\") - vector(\"Man\") + vector(\"Woman\") = vector(\"Queen\")__. They tested this by creating a large test set of 5 types of semtantic relationships and 9 types of syntactic questions. This was done by manually creating a list of word pairs, for example __(Athens, Greece)__ or __(great, greater)__. Then, each of these word pairs was connected to another word word pair in the same syntactic/semantic category, and the evaluation task consisted of seeing if the embeddings could accurately predict the second word in the second pair, given the relationship computed from the first pair. For example, given __(Athens, Greece)__ $\\to$ __(Oslo, Norway)__, they computed the vectors for each of the first 3 words, did the vector math, and saw whether the result was the correct last word: __vector(Athens) - vector(Greece) + vector(Oslo) =? vector(Norway)__.\n",
    "\n",
    "### 4. What are PCA and t-SNE? Why are these important to the task of training and interpreting word embeddings?\n",
    "\n",
    "PCA (or Principal Component Analysis) is a method of finding the set of dimensions in a vector space which maximizes the variance in each dimension. This is done by taking linear combinations of our other features to try to find the first PC dimension which maximizes the variance of our data. Then, each subsequent PCA dimension is calculated using the same method, with the added condition that the new dimension must be orthogonal to all other PC dimensions. This effectively is a way of projecting the data from $\\mathbb{R}^{N} \\to \\mathbb{R}^{N}$, where the new vector space's axes are sorted in order of \"importance\" or information. To use PCA to visualize word embeddings, we can run PCA on our embedding space, and then take the first 2 or 3 PCA dimensions and use them to visualize our embeddings in a 2 or 3 dimensional space.\n",
    "\n",
    "On the other hand, t-SNE is a method for visualizing high-dimensional data while also trying to avoid the \"curse of dimensionality\". When our data lives in high-dimensional vector spaces, simply projecting the space onto a 2 or 3 dimensional plane will cause our data to overcrowd. This makes it very hard to see clusters or any other meaningful relationship between the data. t-SNE solves this problem by modeling the distribution of points in the high-dimensional space, and then re-creating that distribution in a lower dimensional space. The points in 2-d are sampled according to the distribution, meaning that the user of t-SNE gets to control how many points are sampled. This will avoid the overcrowding problem and will still allow us to visualize our high-dimensional data.\n",
    "\n",
    "PCA and t-SNE are important in training and interpreting word embeddings because they provide a human-interpretable view on generated vector embeddings, which are otherwise long lists of numbers with no inherent semantic meaning to the human eye. Projecting vector embeddings into smaller-dimensional space would potentially reveal clusters in 2 or 3 dimensions which will allow us to visually inspect the quality of our embeddings: does each cluster contain words that we as humans would consider \"similar\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "# The Spooky Author Dataset\n",
    "\n",
    "We first examine the aptly named Spooky Authors dataset. This is a compilation of works from Edgar Allan Poe, H.P. Lovecraft, and Mary Shelley, and can be accessed on Kaggle [here](https://www.kaggle.com/c/spooky-author-identification/overview). \n",
    "\n",
    "# Our Dataset: [insert]\n",
    "\n",
    "__Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and it's general properties.__\n",
    "\n",
    "__(describe your dataset here)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext wurlitzer\n",
    "\n",
    "# Some constants\n",
    "EMBEDDINGS_SIZE = 300\n",
    "SPOOKY_DATA_PATH_TEST = \"./data_files/spooky_test.csv\"\n",
    "SPOOKY_DATA_PATH_TRAIN = \"./data_files/spooky_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "with open(SPOOKY_DATA_PATH_TRAIN, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()  # skip the column names \n",
    "    train = [row[1] for row in reader]  # Take only the text data\n",
    "    \n",
    "with open(SPOOKY_DATA_PATH_TEST, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()  # skip the column names\n",
    "    test = [row[1] for row in reader]  # Take only the text data\n",
    "    \n",
    "# Use all the data\n",
    "sentences = train + test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing\n",
    "--------------------------------\n",
    "\n",
    "Describe []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing/cleaning\n",
    "\n",
    "# Lowercases all words, removes punctuation, \n",
    "# and replaces all instances of a number with NUMBER_TOKEN\n",
    "def clean_data(sentences):\n",
    "    regex_punctuation = r'[.,;:/\\\\\\(\\)\\[\\]!?<>]'\n",
    "    regex_numbers = r'[0-9]'\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tmp = sent.lower()\n",
    "        tmp = re.sub(regex_punctuation, '', tmp)\n",
    "        tmp = re.sub(regex_numbers, '', tmp)\n",
    "        output.append(tmp)\n",
    "    \n",
    "    return output\n",
    "\n",
    "sentences = clean_data(sentences)\n",
    "sentences_listed = [sentence.split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 27971\n",
      "Average sentence length: 26.63 words\n",
      "Total number of words: 744881\n",
      "Average word length 4.495 characters\n",
      "Vocabulary size: 31851\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "\n",
    "# Prints some basic statistics about our dataset:\n",
    "# 1. Total Number of Sentences\n",
    "# 2. Average Sentence Length in Words\n",
    "# 3. Total Number of Words\n",
    "# 4. Average Word Length in Characters\n",
    "# 5. Vocabulary Size\n",
    "# (The function returns the list of all words in the dataset)\n",
    "def print_dataset_statistics(sentences):\n",
    "    num_sentences = len(sentences)\n",
    "    avg_sent_length = np.mean([len(sent.split()) for sent in sentences])\n",
    "    all_words = []\n",
    "    for sent in sentences:\n",
    "        all_words.extend(sent.split())\n",
    "    avg_word_length = np.mean([len(word) for word in all_words])\n",
    "    num_words = len(all_words)\n",
    "    vocab_size = len(set(all_words))\n",
    "    \n",
    "    print(f\"Total sentences: {num_sentences}\")\n",
    "    print(f\"Average sentence length: {round(avg_sent_length, 3)} words\")\n",
    "    print(f\"Total number of words: {num_words}\")\n",
    "    print(f\"Average word length {round(avg_word_length, 3)} characters\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "\n",
    "all_words = print_dataset_statistics(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Train embedding on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "<class 'dict'>\n",
      "Word most similar to 'king': princess\n",
      "Word most similar to 'horse': coat\n",
      "Word most similar to 'man': woman\n",
      "King - Man + Woman: princess\n"
     ]
    }
   ],
   "source": [
    "# Create the embeddings, save the vectors, toss the model\n",
    "model = Word2Vec(sentences=sentences_listed, size=EMBEDDINGS_SIZE, sg=1, window=5, min_count=1)\n",
    "spooky_embeddings = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word most similar to 'king': princess\n",
      "Word most similar to 'horse': coat\n",
      "Word most similar to 'man': woman\n",
      "King - Man + Woman: princess\n",
      "Was the model able to distinguish 'peasant' from food? True\n",
      "Was the model able to distinguish 'favorite' from weekdays? True\n"
     ]
    }
   ],
   "source": [
    "# This function prints some quick similarities and comparisons for the given embeddings\n",
    "def quick_evaluate_embeddings(embeddings):\n",
    "    \n",
    "    similar_to_king = spooky_embeddings.most_similar('king')[0][0]\n",
    "    similar_to_horse = spooky_embeddings.most_similar('horse')[0][0]\n",
    "    similar_to_man = spooky_embeddings.most_similar('man')[0][0]\n",
    "    king_minus_man_plus_woman = spooky_embeddings.most_similar(positive=['king', 'woman'], negative=['man'])[0][0]\n",
    "    odd_one_out_food = spooky_embeddings.doesnt_match(('bread', 'meat', 'vegetables', 'peasant'))\n",
    "    odd_one_out_days = spooky_embeddings.doesnt_match(('monday', 'wednesday', 'friday', 'favorite'))\n",
    "\n",
    "    print(f\"Word most similar to 'king': {similar_to_king}\")\n",
    "    print(f\"Word most similar to 'horse': {similar_to_horse}\")\n",
    "    print(f\"Word most similar to 'man': {similar_to_man}\")\n",
    "    print(f\"King - Man + Woman: {king_minus_man_plus_woman}\")\n",
    "    print(f\"Was the model able to distinguish 'peasant' from food? {odd_one_out_food is 'peasant'}\")\n",
    "    print(f\"Was the model able to distinguish 'favorite' from weekdays? {odd_one_out_days is 'favorite'}\")\n",
    "    \n",
    "\n",
    "quick_evaluate_embeddings(spooky_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of the matrix: (31851, 300)\n",
      "New shape of the matrix: (31851, 50)\n",
      "Cumulative explained variance of first 50 PC dimensions: 99.29999709129333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing t-SNE using 1 cores.\n",
      "Using no_dims = 2, perplexity = 45.000000, and theta = 0.500000\n",
      "Computing input similarities...\n",
      "Building tree...\n",
      " - point 3185 of 31851\n",
      " - point 6370 of 31851\n",
      " - point 9555 of 31851\n",
      " - point 12740 of 31851\n",
      " - point 15925 of 31851\n",
      " - point 19110 of 31851\n",
      " - point 22295 of 31851\n",
      " - point 25480 of 31851\n",
      " - point 28665 of 31851\n",
      " - point 31850 of 31851\n",
      "Done in 9.00 seconds (sparsity = 0.006699)!\n",
      "Learning embedding...\n",
      "Iteration 51: error is 104.302569 (50 iterations in 17.00 seconds)\n",
      "Iteration 101: error is 94.891070 (50 iterations in 15.00 seconds)\n",
      "Iteration 151: error is 90.898851 (50 iterations in 15.00 seconds)\n",
      "Iteration 201: error is 89.874025 (50 iterations in 17.00 seconds)\n",
      "Iteration 251: error is 89.468913 (50 iterations in 19.00 seconds)\n",
      "Iteration 301: error is 3.896709 (50 iterations in 14.00 seconds)\n",
      "Iteration 351: error is 3.631698 (50 iterations in 16.00 seconds)\n",
      "Iteration 401: error is 3.478863 (50 iterations in 12.00 seconds)\n",
      "Iteration 451: error is 3.374528 (50 iterations in 12.00 seconds)\n",
      "Iteration 500: error is 3.298539 (50 iterations in 12.00 seconds)\n",
      "Fitting performed in 149.00 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the embeddings\n",
    "\n",
    "# This is the V x E matrix; the full-length embedding for each vocab word\n",
    "# Each unique word $w_{i}$ represents the row $V_{i}$\n",
    "# Each row is a vector of length E which corresponds to that word's embedding \n",
    "X = spooky_embeddings[spooky_embeddings.vocab]\n",
    "print(f\"Original shape of the matrix: {X.shape}\")  # (31851, 300)\n",
    "\n",
    "\n",
    "# We first run PCA and keep the first num_components dimensions before we pass into t-SNE\n",
    "num_components = 50\n",
    "random_seed = 100\n",
    "\n",
    "pca = PCA(n_components=num_components)\n",
    "X = pca.fit_transform(X)\n",
    "print(f\"New shape of the matrix: {X.shape}\")  # (31851, 50)\n",
    "\n",
    "# Since we ran PCA and kept only the first few dimensions, we want to know how much\n",
    "# of the original information is still preserved in this projection. The concept of\n",
    "# \"explained variance\" is helpful here. Our original 300-dimensional vector space has \n",
    "# a certain amount of variance among the data. This is our \"total variance\". When we \n",
    "# project to 50 dimensions, we lose some of that variance, since we lose dimensions along\n",
    "# which our data can vary. We calculate the total amount of variance in our 50-dimensional\n",
    "# space and divide that number by our total variance. This yields the amount of variance \n",
    "# preserved after our projection. So while our data might not have __all__ of the original\n",
    "# variance, we can still see that it has a significant percentage of the total variance, and\n",
    "# so we only lose a small amount of information while reducing our space a lot. Thanks PCA!\n",
    "print(f\"Cumulative explained variance of first {num_components} PC dimensions: {100 * round(np.sum(pca.explained_variance_ratio_), 3)}%\")\n",
    "\n",
    "\n",
    "# Now we run t-SNE to visualize the clusters\n",
    "tsne = TSNE(n_components=2, perplexity=45, verbose=True, random_state=random_seed, n_iter=500)\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "vis_x = X_embedded[:, 0]\n",
    "vis_y = X_embedded[:, 1]\n",
    "plt.scatter(vis_x, vis_y, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What text-normalization and pre-processing did you do and why? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "\n",
    "NGRAM = 4 # The size of the ngram language model you want to train\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "# Make sure to include the padding token in your vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:54.373208Z",
     "start_time": "2020-10-24T03:27:54.369835Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed ngram based sequences (Used for Feedforward)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples():\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and generates the training samples \n",
    "    out of it.\n",
    "    Parameters:\n",
    "    up to you!\n",
    "    return: list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "    '''Loads and parses embeddings trained in earlier.'''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':1, ...}\n",
    "    # index to embedding : {1:'the', ...} (inverse of word_2_embedding)\n",
    "    pass\n",
    "\n",
    "\n",
    "# remember that \"0\" index is assigned for padding token. \n",
    "# Hence, initialize the vector for padding token as all zeros of embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_generator(X, y, num_sequences_per_batch):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "# num_sequences_per_batch = 1024 # or Batch Size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:56:19.207252Z",
     "start_time": "2020-10-24T04:56:19.204894Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    }
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two \n",
    "\n",
    "# Defining the model architecture using Keras Sequential API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d5fb65f744d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit_generator(train_generator, \n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     epochs=1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Start training the model\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed, n_words):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
