# About
In this notebook, we will be implementing Predictive Text using two different datasets. This project serves to explore the effects of the genre of a dataset on language modeling - does the genre of the dataset influence what word will come next?

We compare two datasets: the first is the Spooky Authors dataset which features creepy stories from Poe, Lovecraft, and Shelley. The second is the complete works of Nietzsche, which contains more philosophical text.

The Neural Language Model is a model which uses N-grams of size N to model language. It is a neural network that takes as input a representation of the N-1 context words, which will be the concatenated result of the word embeddings of the context words. These features are propagated forward through some hidden layers, and output a vector of the length of the vocabulary. This output vector contains a 1 in the position corresponding to the predicted next word's index, and contains 0's for all other elements - a one-hot encoding.

This is a self-supervised problem, since the data we use is text. Based on our N-gram size, we define a context window of size N-1, and a target word. The words that appear in the context window are the predictors, and the next word is the target. Once that data point had been used for training, the next data point is generated by shifting over the context window and the target word one word to the right - essentially considering equal sized chunks of the text at a time.

# Data
[Link to Spooky Authors Dataset](https://www.kaggle.com/c/spooky-author-identification)
[Link to Nietzsche Complete Works](http://www.gutenberg.org/files/58025/58025-h/58025-h.htm)
