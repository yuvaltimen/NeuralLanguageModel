{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Model with Word Embeddings\n",
    "\n",
    "This project serves to explore the effects of the genre of a dataset on language modeling. We compare two datasets: the first is the Spooky Authors dataset which features creepy stories from Poe, Lovecraft, and Shelley. The second is the complete works of Nietzsche, which contains more philosophical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Some constants\n",
    "EMBEDDINGS_SIZE = 300\n",
    "NGRAM_SIZE = 4\n",
    "SPOOKY_DATA_PATH_TEST = \"./data_files/spooky_test.csv\"\n",
    "SPOOKY_DATA_PATH_TRAIN = \"./data_files/spooky_train.csv\"\n",
    "NIETZSCHE_DATA_PATH = \"./data_files/nietzsche_sentences.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "with open(SPOOKY_DATA_PATH_TRAIN, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()  # skip the column names \n",
    "    train = [row[1] for row in reader]  # Take only the text data\n",
    "    \n",
    "with open(SPOOKY_DATA_PATH_TEST, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()  # skip the column names\n",
    "    test = [row[1] for row in reader]  # Take only the text data\n",
    "    \n",
    "# Use all the spooky data\n",
    "sentences_spooky = train + test\n",
    "\n",
    "# Read in the Nietzsche data\n",
    "with open(NIETZSCHE_DATA_PATH, 'r') as f:\n",
    "  # read the lines into a list\n",
    "  sentences_niet = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We first clean and then organize our data into a list-of-list-of strings. Each inner list represents a sentence.\n",
    "\n",
    "The input to the data cleaning step (clean_data) is a list of sentence strings.\n",
    "\n",
    "To clean a sentence, we first change it to entirely lowercase. Next, we remove all punctuation and numbers, replacing them with a whitespace. Then, we remove quotation marks and replace them with whitespace, being careful to preserve any single quotes that are part of contractions.\n",
    "\n",
    "After cleaning the sentence strings, we apply Python's .split() operator to each sentence in the list, resulting in a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing/cleaning\n",
    "\n",
    "# Lowercases all words, removes punctuation, \n",
    "# and replaces all instances of a number with whitespace\n",
    "def clean_data(sentences):\n",
    "    regex_punctuation = r'([.,;:_\"\\/\\\\\\(\\)\\[\\]!?<>]|--)'\n",
    "    regex_numbers = r'[0-9]'\n",
    "    \n",
    "    # Regexes matching a single quote that:\n",
    "    # - precedes a word \n",
    "    # - follows a word\n",
    "    # - is not next to a word\n",
    "    regex_single_quote_start = \"([^a-zA-Z])\\\\'([a-zA-Z])\"\n",
    "    regex_single_quote_end = \"([a-zA-Z])\\\\'([^a-zA-Z])\"\n",
    "    regex_single_quote_standalone = \"([^a-zA-Z])\\\\'([^a-zA-Z])\"\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tmp = sent.lower()\n",
    "        tmp = re.sub(regex_punctuation, ' ', tmp)\n",
    "        tmp = re.sub(regex_numbers, ' ', tmp)\n",
    "        tmp = re.sub(regex_single_quote_start, r'\\1 \\2', tmp)\n",
    "        tmp = re.sub(regex_single_quote_end, r'\\1 \\2', tmp)\n",
    "        tmp = re.sub(regex_single_quote_standalone, r'\\1 \\2', tmp)\n",
    "        output.append(tmp)\n",
    "    \n",
    "    return output\n",
    "\n",
    "sentences_spooky = clean_data(sentences_spooky)\n",
    "sentences_niet = clean_data(sentences_niet)\n",
    "\n",
    "# Turn each sentence into a list of words\n",
    "sentences_listed_spooky = [sentence.split() for sentence in sentences_spooky]\n",
    "sentences_listed_niet = [sentence.split() for sentence in sentences_niet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Statistics\n",
    "We generate some basic statistics about the two corpuses, such as sentence and word count, sentence and word length, and vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Spooky dataset statistics---\n",
      "Total sentences: 27971\n",
      "Average sentence length: 26.633 words\n",
      "Total number of words: 744958\n",
      "Average word length 4.483 characters\n",
      "Vocabulary size: 29027\n",
      "\n",
      "---Nietzsche dataset statistics---\n",
      "Total sentences: 103741\n",
      "Average sentence length: 8.615 words\n",
      "Total number of words: 893702\n",
      "Average word length 4.647 characters\n",
      "Vocabulary size: 31371\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "\n",
    "# Prints some basic statistics about our dataset:\n",
    "# 1. Total Number of Sentences\n",
    "# 2. Average Sentence Length in Words\n",
    "# 3. Total Number of Words\n",
    "# 4. Average Word Length in Characters\n",
    "# 5. Vocabulary Size\n",
    "# (The function returns the list of all words in the dataset)\n",
    "def print_dataset_statistics(sentences):\n",
    "    num_sentences = len(sentences)\n",
    "    avg_sent_length = np.mean([len(sent.split()) for sent in sentences])\n",
    "    all_words = []\n",
    "    for sent in sentences:\n",
    "        all_words.extend(sent.split())\n",
    "    avg_word_length = np.mean([len(word) for word in all_words])\n",
    "    num_words = len(all_words)\n",
    "    vocab_size = len(set(all_words))\n",
    "    \n",
    "    print(f\"Total sentences: {num_sentences}\")\n",
    "    print(f\"Average sentence length: {round(avg_sent_length, 3)} words\")\n",
    "    print(f\"Total number of words: {num_words}\")\n",
    "    print(f\"Average word length {round(avg_word_length, 3)} characters\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "print(\"---Spooky dataset statistics---\")\n",
    "all_words_spooky = print_dataset_statistics(sentences_spooky)\n",
    "print(\"\\n---Nietzsche dataset statistics---\")\n",
    "all_words_niet = print_dataset_statistics(sentences_niet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings\n",
    "We create separate word embeddings for each dataset using word2vec. Then, we evaluate the differences between these embeddings in two ways:\n",
    "\n",
    "* Finding most-similar words to common words, and finding the least-similar word from a group of words, all but one of which share a category.\n",
    "\n",
    "* Visualize these two embeddings using TSNE to project the data down to 2 dimensions. We highlight the positions of specific related words in the graphs to evaluate the differences between relative positions of related words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings, save the KeyedVector, toss the model\n",
    "model_spooky = Word2Vec(sentences=sentences_listed_spooky, size=EMBEDDINGS_SIZE, sg=1, window=5, min_count=1)\n",
    "model_niet = Word2Vec(sentences=sentences_listed_niet, size=EMBEDDINGS_SIZE, sg=1, window=5, min_count=1)\n",
    "\n",
    "spooky_embeddings = model_spooky.wv\n",
    "niet_embeddings = model_niet.wv\n",
    "\n",
    "del model_spooky\n",
    "del model_niet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for Spooky Embeddings:\n",
      "Word most similar to 'king': title\n",
      "Word most similar to 'horse': knife\n",
      "Word most similar to 'man': woman\n",
      "King - Man + Woman: widow\n",
      "Was the model able to distinguish 'peasant' from food? True\n",
      "Was the model able to distinguish 'favorite' from weekdays? True\n",
      "Was the model able to distinguish 'train' from animals? False\n",
      "--------------------------------------\n",
      "Output for Nietzsche Embeddings\n",
      "Word most similar to 'king': soothsayer\n",
      "Word most similar to 'horse': four-and-fortieth\n",
      "Word most similar to 'man': thinker\n",
      "King - Man + Woman: wrote\n",
      "Was the model able to distinguish 'peasant' from food? False\n",
      "Was the model able to distinguish 'favorite' from weekdays? True\n",
      "Was the model able to distinguish 'train' from animals? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "# This function prints some quick similarities and comparisons for the given embeddings\n",
    "def quick_evaluate_embeddings(embeddings):\n",
    "    \n",
    "    similar_to_king = embeddings.most_similar('king')[0][0]\n",
    "    similar_to_horse = embeddings.most_similar('horse')[0][0]\n",
    "    similar_to_man = embeddings.most_similar('man')[0][0]\n",
    "    king_minus_man_plus_woman = embeddings.most_similar(positive=['king', 'woman'], negative=['man'])[0][0]\n",
    "    odd_one_out_food = embeddings.doesnt_match(('bread', 'meat', 'vegetables', 'peasant'))\n",
    "    odd_one_out_days = embeddings.doesnt_match(('monday', 'wednesday', 'friday', 'favorite'))\n",
    "    odd_one_out_animals = embeddings.doesnt_match(('cat', 'dog', 'sheep', 'wolf', 'hose'))\n",
    "\n",
    "    print(f\"Word most similar to 'king': {similar_to_king}\")\n",
    "    print(f\"Word most similar to 'horse': {similar_to_horse}\")\n",
    "    print(f\"Word most similar to 'man': {similar_to_man}\")\n",
    "    print(f\"King - Man + Woman: {king_minus_man_plus_woman}\")\n",
    "    print(f\"Was the model able to distinguish 'peasant' from food? {odd_one_out_food == 'peasant'}\")\n",
    "    print(f\"Was the model able to distinguish 'favorite' from weekdays? {odd_one_out_days == 'favorite'}\")\n",
    "    print(f\"Was the model able to distinguish 'train' from animals? {odd_one_out_animals == 'train'}\")\n",
    "    \n",
    "\n",
    "print('Output for Spooky Embeddings:')\n",
    "quick_evaluate_embeddings(spooky_embeddings)\n",
    "print('--------------------------------------')\n",
    "print('Output for Nietzsche Embeddings')\n",
    "quick_evaluate_embeddings(niet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of the spooky matrix: (29027, 300)\n",
      "Original shape of the philosophical matrix: (31371, 300)\n",
      "New shape of the spooky matrix: (29027, 50)\n",
      "New shape of the philosophical matrix: (31371, 50)\n",
      "SPOOKY: Cumulative explained variance of first 50 PC dimensions: 99.19999837875366%\n",
      "PHILOSOPHICAL: Cumulative explained variance of first 50 PC dimensions: 98.19999933242798%\n"
     ]
    }
   ],
   "source": [
    "# Building the t-SNE projection\n",
    "\n",
    "# This is the V x E matrix; the full-length embedding for each vocab word\n",
    "# Each unique word w_i represents the row V_i\n",
    "# Each row is a vector of length E which corresponds to that word's embedding \n",
    "embedding_matrix_spooky = spooky_embeddings[spooky_embeddings.vocab]\n",
    "embedding_matrix_niet = niet_embeddings[niet_embeddings.vocab]\n",
    "\n",
    "print(f\"Original shape of the spooky matrix: {embedding_matrix_spooky.shape}\")  # (31851, 300)\n",
    "print(f\"Original shape of the philosophical matrix: {embedding_matrix_niet.shape}\")  # (31851, 300)\n",
    "\n",
    "\n",
    "# We first run PCA and keep the first num_components dimensions before we pass into t-SNE\n",
    "num_components = 50\n",
    "random_seed = 100\n",
    "\n",
    "pca_spooky = PCA(n_components=num_components)\n",
    "pca_embeddings_spooky = pca_spooky.fit_transform(embedding_matrix_spooky)\n",
    "pca_niet = PCA(n_components=num_components)\n",
    "pca_embeddings_niet = pca_niet.fit_transform(embedding_matrix_niet)\n",
    "\n",
    "print(f\"New shape of the spooky matrix: {pca_embeddings_spooky.shape}\")  # (31851, 50)\n",
    "print(f\"New shape of the philosophical matrix: {pca_embeddings_niet.shape}\")  # (31851, 50)\n",
    "\n",
    "# Since we ran PCA and kept only the first few dimensions, we want to know how much\n",
    "# of the original information is still preserved in this projection. The concept of\n",
    "# \"explained variance\" is helpful here. Our original 300-dimensional vector space has \n",
    "# a certain amount of variance among the data. This is our \"total variance\". When we \n",
    "# project to 50 dimensions, we lose some of that variance, since we lose dimensions along\n",
    "# which our data can vary. We calculate the total amount of variance in our 50-dimensional\n",
    "# space and divide that number by our total variance. This yields the amount of variance \n",
    "# preserved after our projection. So while our data might not have *all* of the original\n",
    "# variance, we can still see that it has a significant percentage of the total variance, and\n",
    "# so we only lose a small amount of information while reducing our space a lot.\n",
    "print(f\"SPOOKY: Cumulative explained variance of first {num_components} PC dimensions: {100 * round(np.sum(pca_spooky.explained_variance_ratio_), 3)}%\")\n",
    "print(f\"PHILOSOPHICAL: Cumulative explained variance of first {num_components} PC dimensions: {100 * round(np.sum(pca_niet.explained_variance_ratio_), 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 136 nearest neighbors...\n",
      "[t-SNE] Indexed 29027 samples in 0.660s...\n",
      "[t-SNE] Computed neighbors for 29027 samples in 240.584s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 29027\n",
      "[t-SNE] Computed conditional probabilities for sample 29027 / 29027\n",
      "[t-SNE] Mean sigma: 0.042703\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 89.035713\n",
      "[t-SNE] KL divergence after 500 iterations: 3.142432\n",
      "[t-SNE] Computing 136 nearest neighbors...\n",
      "[t-SNE] Indexed 31371 samples in 0.647s...\n",
      "[t-SNE] Computed neighbors for 31371 samples in 294.000s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 31371\n",
      "[t-SNE] Computed conditional probabilities for sample 31371 / 31371\n",
      "[t-SNE] Mean sigma: 0.027828\n"
     ]
    }
   ],
   "source": [
    "# Now we run t-SNE\n",
    "tsne_spooky = TSNE(n_components=2, perplexity=45, verbose=True, random_state=random_seed, n_iter=500)\n",
    "tsne_embeddings_spooky = tsne_spooky.fit_transform(embedding_matrix_spooky)\n",
    "\n",
    "tsne_niet = TSNE(n_components=2, perplexity=45, verbose=True, random_state=random_seed, n_iter=500)\n",
    "tsne_embeddings_niet = tsne_niet.fit_transform(embedding_matrix_niet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "\n",
    "vis_x_spooky = tsne_embeddings_spooky[:, 0]\n",
    "vis_y_spooky = tsne_embeddings_spooky[:, 1]\n",
    "\n",
    "vis_x_niet = tsne_embeddings_niet[:, 0]\n",
    "vis_y_niet = tsne_embeddings_niet[:, 1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(16, 10))\n",
    "\n",
    "ax1.scatter(vis_x_spooky, vis_y_spooky, c='grey', marker='.')\n",
    "ax2.scatter(vis_x_niet, vis_y_niet, c='grey', marker='.')\n",
    "\n",
    "\n",
    "def visualize_clusters(current_ax, embeddings, tsne_model):\n",
    "    # We highlight the location of several words of interest on this chart.\n",
    "    words_of_interest = ('man', 'woman', 'boy', 'girl', 'child', 'adult',\n",
    "                         'coat', 'hat',  'glove', 'gloves',\n",
    "                         'dog', 'horse', 'horses', 'cow',\n",
    "                         'large', 'big', 'tiny', 'small', 'smaller', 'smallest', \n",
    "                        'walk', 'walks', 'speak', 'spoke', 'eat', 'ate')\n",
    "    vectors_of_interest = []\n",
    "    # Need to look up the index of the word to find its corresponding t-SNE vector\n",
    "    for word in words_of_interest:\n",
    "        idx_of_word = embeddings.vocab[word].index\n",
    "        vectors_of_interest.append(tsne_model.embedding_[idx_of_word])\n",
    "\n",
    "    # Text styling parameters\n",
    "    font= {'family':'serif','color':'black','weight':'normal','size':14}\n",
    "\n",
    "    # Draw the words of interest on and label them\n",
    "    for idx, vec in enumerate(vectors_of_interest):\n",
    "        current_ax.scatter(vec[0], vec[1], c='yellow', marker='x')\n",
    "        current_ax.text(vec[0], vec[1], words_of_interest[idx], fontdict=font)\n",
    "\n",
    "        \n",
    "visualize_clusters(ax1, spooky_embeddings, tsne_spooky)\n",
    "visualize_clusters(ax2, niet_embeddings, tsne_niet)\n",
    "ax1.set_title('Spooky embeddings')\n",
    "ax2.set_title('Nietzsche embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model with Feedforward Neural Network\n",
    "We will first encode our texts into integers to make it easier to create one-hot encodings.\n",
    "Given the previous N-1 context words, the model will attempt to predict the Nth word. So the input to our neural network will need to be a representation of the N-1 context words. Since we are using N-gram size of N=4, then given 3 context words, the model must predict the 4th word. We represent the words using their word embeddings. \n",
    "\n",
    "The input to the model will be a vector of shape (900,) which will be the result of concatenating the word embeddings of the 3 context words together. The output dimension of our network will be |V|, the size of the vocabulary. The model will output the one-hot encoding of the predicted 4th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we assign each word in the vocabulary a unique index\n",
    "# We then print the outputs for a sanity check.\n",
    "tokenizer_spooky = Tokenizer()\n",
    "tokenizer_spooky.fit_on_texts(sentences_listed_spooky)\n",
    "vocab_size_spooky = len(list(tokenizer_spooky.word_index.keys())) + 1\n",
    "print(f\"First 5 words: {list(tokenizer_spooky.word_index.keys())[:5]}\")\n",
    "\n",
    "tokenizer_niet = Tokenizer()\n",
    "tokenizer_niet.fit_on_texts(sentences_listed_niet)\n",
    "vocab_size_niet = len(list(tokenizer_niet.word_index.keys()))\n",
    "print(f\"First 5 words: {list(tokenizer_niet.word_index.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be feeding into our model a sequence of vectors, each of shape (900,), \n",
    "# representing the concatenated word embeddings of the 3 context words. The output will be \n",
    "# a one-hot encoding of the predicted word. In order to train our model, we will format our\n",
    "# data in this format. To do this, we will create a data generator that yields a tuple (X, y), \n",
    "# where X is the (900,) vector of word embeddings and y is the one-hot encoded output word.\n",
    "\n",
    "\n",
    "# This function returns the concatenated word embeddings of the supplied words\n",
    "# It takes in a list of context words and an embedding mapping, and returns the\n",
    "# concatenated result. \n",
    "# Inputs:\n",
    "#    context (list)                          - the list of length (n_gram - 1) context words\n",
    "#    embeddings (gensim.models.KeyedVectors) - the mapping from word -> word embeddings used to represent the words\n",
    "# Returns:\n",
    "#    numpy array of shape (n_gram - 1) x embedding_size, ie. (900,)\n",
    "def concat_embeddings(context, embeddings):\n",
    "    embedded_contexts = [embeddings.get_vector(word) for word in context]\n",
    "    concatted = np.concatenate(embedded_contexts)\n",
    "    return np.reshape(concatted, newshape=(1, 900))\n",
    "    \n",
    "\n",
    "# This function returns the one-hot representation of the supplied word.\n",
    "# Inputs:\n",
    "#    word (str)                                     - the word to convert to one-hot encoding\n",
    "#    tokenizer (keras.preprocessing.text.Tokenizer) - the mapping from word -> index used to encode the sentneces\n",
    "# Returns:\n",
    "#    numpy array of shape |V|, ie. the size of the vocabulary\n",
    "def to_onehot(word, tokenizer):\n",
    "    idx = tokenizer.word_index[word]\n",
    "    vocab_size = len(list(tokenizer.word_index.keys())) + 1\n",
    "    onehot = to_categorical([[idx]], num_classes=vocab_size)\n",
    "    return np.reshape(onehot, newshape=(onehot.shape[0], onehot.shape[-1]))\n",
    "\n",
    "\n",
    "# This is the data generator that yields our data as an X,y pair. \n",
    "# Inputs:\n",
    "#   data (list)                                    - the list of list of words, ie. the sentences\n",
    "#   n_gram (int)                                   - the n-gram order to be used in the neural language model\n",
    "#   tokenizer (keras.preprocessing.text.Tokenizer) - the mapping from word -> index used to encode the sentneces\n",
    "#   embeddings (gensim.models.KeyedVectors)        - the mapping from word -> word embeddings used to represent the words\n",
    "# Yields:\n",
    "#   tuple - (X, y)\n",
    "#   X: numpy array of shape (900,)\n",
    "#   y: numpy array of shape (|V|,)\n",
    "def data_generator(data, n_gram, tokenizer, embeddings):\n",
    "    for sent in data:\n",
    "        # We need at least n_gram words to run the model\n",
    "        # If we don't have enough, ignore the current sentence\n",
    "        if len(sent) < n_gram:\n",
    "            continue\n",
    "        # For a sentence of length S, we will have (S - n_gram) outputs from the generator\n",
    "        for i in range(len(sent) + 1 - n_gram):\n",
    "            # Define the context words\n",
    "            context_words = sent[i : i + n_gram - 1]\n",
    "            # Define the word we are trying to predict\n",
    "            pred_word = sent[i + n_gram - 1]\n",
    "            # Get the concatenated word embeddings for our context words\n",
    "            embedded_context_words = concat_embeddings(context_words, embeddings)\n",
    "            # Get the one-hot encoding of our context word\n",
    "            onehot_pred_word = to_onehot(pred_word, tokenizer)\n",
    "            # Yield the data\n",
    "            yield embedded_context_words, onehot_pred_word\n",
    "            \n",
    "\n",
    "# Wrapper class for the data generator defined above\n",
    "# Subclasses keras.utils.Sequence, allowing us to reset the generator after each epoch\n",
    "# Inputs:\n",
    "#   data (list)                                    - the list of list of words, ie. the sentences\n",
    "#   n_gram (int)                                   - the n-gram order to be used in the neural language model\n",
    "#   tokenizer (keras.preprocessing.text.Tokenizer) - the mapping from word -> index used to encode the sentneces\n",
    "#   embeddings (gensim.models.KeyedVectors)        - the mapping from word -> word embeddings used to represent the words\n",
    "# Yields:\n",
    "#   tuple - (X, y)\n",
    "#   X: numpy array of shape (900,)\n",
    "#   y: numpy array of shape (|V|,)\n",
    "class data_generator_resettable(Sequence):\n",
    "    def __init__(self, data, n_gram, tokenizer, embeddings):\n",
    "        self.data = data\n",
    "        self.n_gram = n_gram\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings = embeddings\n",
    "        self.generator = data_generator(data, n_gram, tokenizer, embeddings)\n",
    "        \n",
    "    # Returns the batch at the given index\n",
    "    # Since each batch will be a single (X, y) pair, we can just return the next item\n",
    "    # If we hit the end of the line, reset the generator and try again\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return next(self.generator)\n",
    "        except StopIteration:\n",
    "            self.on_epoch_end()\n",
    "            return self.__getitem__(index)\n",
    "    \n",
    "    # Returns the length of the entire generator\n",
    "    def __len__(self):\n",
    "        tmp = data_generator(self.data, self.n_gram, self.tokenizer, self.embeddings)\n",
    "        count = 0\n",
    "        going = True\n",
    "        while going:\n",
    "            try:\n",
    "                next(tmp)\n",
    "                count += 1\n",
    "            except StopIteration:\n",
    "                going = False\n",
    "        return count\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.generator = data_generator(self.data, self.n_gram, self.tokenizer, self.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample data to verify the output of the data generator\n",
    "test_data = [['of', 'the', 'we', 'are', 'have'], ['not', 'enough', 'here'], ['is', 'are', 'we', 'is']]\n",
    "data_gen = data_generator_resettable(test_data, NGRAM_SIZE, tokenizer_spooky, spooky_embeddings)\n",
    "first_out = data_gen.__getitem__(0)\n",
    "data_gen.on_epoch_end()\n",
    "print(f\"Shape of X, y: {first_out[0].shape} {first_out[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've verified that the data generator works correctly\n",
    "# We can now define the data generator for our actual data\n",
    "\n",
    "generator_spooky = data_generator_resettable(sentences_listed_spooky, NGRAM_SIZE, tokenizer_spooky, spooky_embeddings)\n",
    "generator_niet = data_generator_resettable(sentences_listed_niet, NGRAM_SIZE, tokenizer_niet, niet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the models\n",
    "neural_lm_spooky = Sequential()\n",
    "neural_lm_spooky.add(Dense(32, name='h1', input_shape=(1,900), activation='tanh'))\n",
    "neural_lm_spooky.add(Dense(50, name='h2', activation='relu'))\n",
    "neural_lm_spooky.add(Dense(vocab_size_spooky, name='output', activation='softmax'))\n",
    "\n",
    "neural_lm_spooky.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(\"Spooky model summary:\")\n",
    "neural_lm_spooky.summary()\n",
    "\n",
    "neural_lm_niet = Sequential()\n",
    "neural_lm_niet.add(Dense(32, name='h1', input_shape=(1, 900), activation='tanh'))\n",
    "neural_lm_niet.add(Dense(50, name='h2', activation='relu'))\n",
    "neural_lm_niet.add(Dense(vocab_size_niet, name='output', activation='softmax'))\n",
    "\n",
    "neural_lm_niet.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(\"Nietzsche model summary:\")\n",
    "neural_lm_niet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "neural_lm_spooky.fit(generator_spooky, verbose=2, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# neural_lm_niet.fit(generator_niet, verbose=2, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now that we have trained our models, we would like to evaluate them. \n",
    "The purpose of a language model is to predict the next word in the sentence, \n",
    "and so we will be comparing the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function will generate a sequence from the model.\n",
    "# Inputs:\n",
    "#    model (keras.Model)                            - the model to be used\n",
    "#    tokenizer (keras.preprocessing.text.Tokenizer) - the mapping from word -> index used to encode the sentneces\n",
    "#    seed (list)                                    - the sequence of N-1 context words from which to begin the sequence\n",
    "#    n_words (int)                                  - the length of the output sequence\n",
    "def generate_seq(model, tokenizer, seed, n_words, embedding):\n",
    "    output = seed\n",
    "    while len(output) < n_words:\n",
    "        i = 0\n",
    "        embedded = concat_embeddings(output[i : i + NGRAM_SIZE], embeddings)\n",
    "#         embedded = np.concatenate([embedding.get_vector(word) for word in output[-NGRAM_SIZE:]])\n",
    "        prob_distr = model.predict(embedded)\n",
    "        next_word_idx = np.argmax(prob_distr)\n",
    "        output.append(tokenizer.word_index[next_word_idx])\n",
    "        i += 1\n",
    "    return output\n",
    "\n",
    "\n",
    "niet_seed = [[\"in\", \"the\", \"belief\"],\n",
    "             [\"we\", \"need\", \"it\"],\n",
    "             [\"tried\", \"to\", \"describe\"],\n",
    "             [\"for\", \"life\", \"and\"],\n",
    "             [\"tell\", \"me\", \"that\"],\n",
    "             [\"great\", \"historical\", \"movement\"],\n",
    "             [\"at\", \"all\", \"costs\"],\n",
    "             [\"i\", \"gain\", \"an\"],\n",
    "             [\"these\", \"thoughts\", \"are\"],\n",
    "             [\"as\", \"a\", \"fault\"],\n",
    "             [\"even\", \"if\", \"it\"],\n",
    "             [\"be\", \"a\", \"virtue\"],\n",
    "             [\"a\", \"child\", \"of\"],\n",
    "             [\"i\", \"must\", \"admit\"],\n",
    "             [\"man\", \"cannot\", \"see\"],\n",
    "             [\"simply\", \"to\", \"live\"],\n",
    "             [\"to\", \"live\", \"without\"],\n",
    "             [\"the\", \"beast\", \"wants\"],\n",
    "             [\"but\", \"he\", \"forgets\"],\n",
    "             [\"i\", \"always\", \"forget\"]]\n",
    "\n",
    "spooky_seed = [[\"if\", \"a\", \"fire\"],\n",
    "               [\"as\", \"i\", \"urged\"],\n",
    "               [\"the\", \"tone\", \"metaphysical\"],\n",
    "               [\"the\", \"offspring\", \"of\"],\n",
    "               [\"persuading\", \"the\", \"widow\"],\n",
    "               [\"when\", \"i\", \"arose\"],\n",
    "               [\"tears\", \"in\", \"my\"],\n",
    "               [\"nothing\", \"was\", \"elicited\"],\n",
    "               [\"the\", \"lost\", \"inheritance\"],\n",
    "               [\"could\", \"almost\", \"see\"],\n",
    "               [\"freak\", \"of\", \"fancy\"],\n",
    "               [\"grapple\", \"not\", \"now\"],\n",
    "               [\"upon\", \"the\", \"figure\"],\n",
    "               [\"restrain\", \"your\", \"impatience\"],\n",
    "               [\"he\", \"is\", \"dead\"],\n",
    "               [\"i\", \"felt\", \"much\"],\n",
    "               [\"and\", \"this\", \"fall\"],\n",
    "               [\"during\", \"this\", \"shocking\"],\n",
    "               [\"was\", \"he\", \"murdered\"],\n",
    "               [\"darkness\", \"had\", \"no\"]]\n",
    "\n",
    "\n",
    "print(\"Spooky sentences:\")\n",
    "print(\"================\")\n",
    "for each_seed in spooky_seed:\n",
    "    print(\" \".join(generate_seq(neural_lm_spooky, tokenizer_spooky, each_seed, 20, spooky_embeddings)))\n",
    "    \n",
    "# print(\"Philosophical sentences:\")\n",
    "# print(\"================\")\n",
    "# for each_seed in niet_seed:\n",
    "#     print(\" \".join(generate_seq(neural_lm_niet, tokenizer_nit, each_seed, 20, niet_embeddings)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
